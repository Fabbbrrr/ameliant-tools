# Kafka Performance Tool Users Guide

This tool provides a standalone mechanism for measuring load and validate the behaviour of Apache Kafka clusters.
Beyond simple performance measurement, it provides a way to understand Kafka through observing its behaviour from the 
outside - after all, observation trumps documentation :)

This tool aims to be more complete than the scripts provided with the Kafka distribution, in allowing you to define
entire test profiles comprising multiple consumers and producers communicating over a set of topics. This is useful 
when simulating the behaviours of multiple functional nodes communicating with each other over Kafka.

## Running

To run this tool execute the JAR as follows (Java 1.8 minimum):

    $ java -jar kafka-perf-tool.jar -c myConfig.yml

Flags are as follows:

    usage: kafka-perf-tool
     -c,--config <FILE>               config file that defines the test
                                      profile(s) to run
     -o,--output-format <yaml|json>   the format of the parsed config to echo
                                      to console

## Test Profiles

To support this functionality each test is driven by a single config file which defines a test profile. This file may 
be defined in YAML or JSON, although the former is generally preferred as it allows for comments. A test profile 
contains within it both Kafka configuration (defined through sets of properties) as well as tool-specific properties 
that define the tool's behaviour. 

Sample configurations may be found in the [test-profiles folder] (../src/test/resources/test-profiles).

The general structure of a test profile is as follows (pseudo-JSON):

    # TestProfileDefinition
    "config" : {} # Kafka configuration applying to all producers and consumers
    "producers" : { # defines any producers as well as their shared config
        "config" : {} # Kafka configuration applying to all producers
        "instances" : [
            # list of producers
            {
                # ProducerDefinition
                "config" : {} # Kafka configuration for this producer
            }
        ]
    }
    "consumers" : { # defines any consumers as well as their global config
        "config" : {} # Kafka configuration applying to all shared
        "instances" : [
            # list of consumers
            {
                # ConsumerDefinition
                "config" : {} # Kafka configuration for this consumer
            }
        ]
    }
    
`config` blocks are simply maps that are passed directly to Kafka when starting up producers and consumers. 
They are not validated by the tool in any way. Configuration cascades down, where `config` defined at the most specific 
level (in a producer or a consumer) inherits and overrides the config of the layer above it, which does the same to the
configuration at the top level.

All other properties are bound to field in the tool's 
[configuration object graph] (../src/main/java/com/ameliant/tools/kafka/perftool/config); the tool will complain if these 
are not correctly defined. Most fields are optional, so if is perfectly reasonable to omit them if not needed; you can 
for example define consumers only, or producers only, or not define `config` at some level.

The top-level element is a `TestProfileDefinition`.

### TestProfileDefinition

* `config` - a map of Kafka properties that apply to all producers and consumers
* `maxDuration` (`30`) - Maximum test duration in seconds. Applies to concurrent tests only.
* `concurrent` (`true`) - Whether producers and consumers execute concurrently. If false, producers will executed before consumers.
* `autogenerateTopic` (`false`) - Whether or not the test should use an auto-generated topic name. If true, consumers and producers will all
use the auto-generated one in preference to any defined within their config during this run.
* `producers` : `ProducersDefinition`
* `consumers` : `ConsumersDefinition`

### ProducersDefinition

* `config` - a map of Kafka properties that apply to all producers
* `topic` - topic for all producers to send to (may be overridded by the producer)
* `instances` : `List[ProducerDefinition]`

### ProducerDefinition

* `config` - a map of Kafka properties for this producer
* `topic` - topic to send to
* `sendDelay` (`0`) - how long in ms to pause between sending messages
* `messagesToSend` (`10000`)
* `messageSize` (`1024`) - how big an autogenerated message should be. A message payload is generated once per producer
 so as to not mess with timings.
* `messageLocation` - Location of a file to use as the message payload. If provided, driver will not generate its own payloads
 and `messageSize` will be ignored. This can be a relative or an absolute path.
* `sendBlocking` (`false`) - whether to wait for acknowledgement from the client library before sending the next message.
 Kafka uses a client buffer, which it consumes from in the background in order to send to the broker.
* `keyAllocationStrategy`:`KeyAllocationStrategyDefinition` (`{"type":"fair", "uniqueKeys":"1"}`) - test functionality 
 that simulates key allocation within your code, allowing you to see how partitioning affects the distribution of sent messages.
* `partitioningStrategy`
    * `none` (default) - Will use Kafka's built in strategy to hash the key into one of the available partitions. 
    This strategy does not give a reliably fair distribution, as many keys potentially hash into the same bucket. 
    * `roundRobin` - This built-in `Partitioner` will round-robin distribute the messages amongst the topic partitions
    without considering the value of key.
    * `sticky` - This built-in `Partitioner` acts in a similar way to [JMS Message Groups] (http://activemq.apache.org/message-groups.html), 
    as it is a form of sticky load-balancing of messages. Each new key is assigned a topic partition on a 
    round-robin basis; subsequent messages sent with the same key will be sent to that same partition. 
    Note that this partitioning is on a per-JVM basis, 
    multiple JVM/test instances running at the same time will likely not partition their keys in the same way.

### KeyAllocationStrategyDefinition

* `type` - built in allocation strategy algorithm, in your own Kafka producers you would assign business-specific keys. This has two options:
    * `fair` - evenly assigns the keys to each message using a modulo function
    * `random` - randomly assigns one of the keys
* `uniqueKeys` - how many keys this strategy should generate for allocation

### ConsumersDefinition

* `config` - a map of Kafka properties that apply to all consumers
* `topic` - topic for all consumers to receive from (may be overridded by the consumer)
* `instances` : `List[ConsumerDefinition]`

### ConsumerDefinition

* `config` - a map of Kafka properties for this consumer
* `topic` - topic to receive from
* `messagesToReceive` (`1000`)
* `pollTimeout` (`1000`) - ms until a call to `poll()` returns.
* `reportReceivedEvery` (`1000`) - how many messages should pass between reporting to the console that that many messages were consumed
* `receiveDelay` (`0`) - how long to pause before processing the next message. Note that this may not mean polling again, 
 as many messages are fetched with each call to `poll()`. 
